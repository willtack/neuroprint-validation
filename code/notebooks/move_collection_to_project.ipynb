{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "074ebe9e-c10c-4502-aaf9-d182158853f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flywheel\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os \n",
    "import glob\n",
    "\n",
    "fw = flywheel.Client()\n",
    "gear = 'deid-export'\n",
    "gear_fw = fw.lookup(f'gears/{gear}')\n",
    "collection_id = '618d6fb6bb845b310730d8f8' # Neuroprint Validation collection\n",
    "project_id = '60fef55e60ec55d1b0e0741e' # Neuroprint Validation project\n",
    "project = fw.get(project_id)\n",
    "collection = fw.get(collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d83dd5-6509-4d19-91de-9b80847a2f0a",
   "metadata": {},
   "source": [
    "## Run gear to move sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ee5246-a3d5-4103-ac5a-3cf035cae3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_sessions = [fw.get_session(x.id) for x in fw.get_collection_sessions(collection_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a7191d-5b07-4f66-ad9b-7f094acf035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ses in collection_sessions:\n",
    "    project = fw.get(ses.parents['project'])\n",
    "    deid_profile_list = [f for f in project.files if 'blank-export' in f.name]\n",
    "    if len(deid_profile_list) > 0:\n",
    "        inputs = {\"deid_profile\": deid_profile_list[0]}\n",
    "        config = {\"project_path\": 'detre_group/Neuroprint Validation', 'overwrite_files': True}\n",
    "        asys_label = 'deid-export_WT_NeuroprintValidation'\n",
    "        analysis_id = gear_fw.run(analysis_label=asys_label, config=config, inputs=inputs, destination=ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5116e-fe62-465e-ba6a-63685db53d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check status real quick\n",
    "# 11/24/2021 - deleted them all so it now shows run=False for all sessions, but that's to be expected\n",
    "data_dict = {'subject':[],'session':[],'run':[], 'status':[]}\n",
    "for ses in collection_sessions:\n",
    "    session = ses.reload()\n",
    "    sub_label = session.subject.label\n",
    "    ses_label = session.label\n",
    "    analyses = session.analyses\n",
    "\n",
    "    if len(analyses) == 0:\n",
    "        run = 'False'\n",
    "    else:\n",
    "        # Loop through the analyses\n",
    "        matches = [asys for asys in analyses if asys.gear_info.get('name') == gear]\n",
    "        if len(matches) == 0:\n",
    "            run = 'False'\n",
    "            status = 'na'\n",
    "        else:\n",
    "            status = matches[0].job.get('state')\n",
    "            run = 'True'\n",
    "            \n",
    "    data_dict['subject'].append(sub_label)\n",
    "    data_dict['session'].append(ses_label)\n",
    "    data_dict['run'].append(run)\n",
    "    data_dict['status'].append(status)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a30dbc-417b-4a4b-91e1-ae143d523039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete gear runs (they'll still remain in provenance, but will be out of sight for other people)\n",
    "for ses in collection_sessions:\n",
    "    for analysis in ses.analyses:\n",
    "        if 'deid-export_WT_NeuroprintValidation' in analysis.label and analysis.job.state=='complete':\n",
    "            fw.delete_session_analysis(ses.id, analysis.id)\n",
    "            print(f\"Deleted {analysis.label} from {ses.label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883ebf7-ad77-4598-adbd-655bd9b48939",
   "metadata": {},
   "source": [
    "## Remove non-T1 acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f94b28c2-a8f8-4652-8a57-634a76a552ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_sessions = [fw.get_session(x.id) for x in fw.get_project_sessions(project_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc016c50-8ef1-45a6-8fe9-50787c9961d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>acquisition</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123894x20190327x3T</td>\n",
       "      <td>T1_3D_0.8x0.8x0.8</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>124822x20210126x3T</td>\n",
       "      <td>T1_3D_0.8x0.8x0.8</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>20101220x1159</td>\n",
       "      <td>t1_mpr_AX_MPRAGE</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>20151007x1556</td>\n",
       "      <td>t1_mpr_AX_MPRAGE</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>20090106x1214</td>\n",
       "      <td>t1_mpr_AX_MPRAGE</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                session        acquisition status\n",
       "0    123894x20190327x3T  T1_3D_0.8x0.8x0.8   kept\n",
       "471  124822x20210126x3T  T1_3D_0.8x0.8x0.8   kept\n",
       "755       20101220x1159   t1_mpr_AX_MPRAGE   kept\n",
       "982       20151007x1556   t1_mpr_AX_MPRAGE   kept\n",
       "750       20090106x1214   t1_mpr_AX_MPRAGE   kept"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_dict = {'session':[], 'acquisition':[],'status':[]}\n",
    "for ses in project_sessions:\n",
    "    acqlist = ses.acquisitions()\n",
    "    for acq in acqlist:        \n",
    "        lab = acq.label.lower()\n",
    "        if (\"vnav\" in lab) and (\"moco\" in lab) and (\"rms\" in lab) and not (\"nd\" in lab) and not ('passive' in lab):\n",
    "            status = 'kept'\n",
    "        elif (\"vnav\" in lab) and (\"rms\" in lab) and not (\"nd\" in lab):\n",
    "            status = 'kept'\n",
    "        elif (\"ax\" in lab) and (\"mprage\" in lab):\n",
    "            status = 'kept'\n",
    "        elif (\"sag\" in lab) and (\"mprage\" in lab):\n",
    "            status = 'kept'\n",
    "        elif (\"t1_3d\" in lab and not (\"nd\" in lab)): #NACC\n",
    "            status = 'kept'\n",
    "        else:\n",
    "            fw.delete_acquisition(acq.id)\n",
    "            status='deleted'\n",
    "\n",
    "        delete_dict['session'].append(ses.label)\n",
    "        delete_dict['acquisition'].append(acq.label)\n",
    "        delete_dict['status'].append(status)\n",
    "        \n",
    "\n",
    "delete_df = pd.DataFrame.from_dict(delete_dict)\n",
    "delete_df = delete_df.sort_values('status', ascending=False) # show what's kept at the top\n",
    "delete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9040d7ee-deb8-4157-9a0e-1d7e8acaa417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a timestamp to our csv name so it won't overwrite anything when we upload it to flywheel\n",
    "time_fmt = '%m-%d-%Y_%H-%M-%S'\n",
    "time_string = datetime.now().strftime(time_fmt)\n",
    "csv_out = f'../run_reports/{gear}_DeletionReport_{time_string}.csv'\n",
    "\n",
    "delete_df.to_csv(csv_out,index=False)\n",
    "\n",
    "project.upload_file(csv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ef870f1-02b3-4ee0-a807-5a1146ca49e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20151007x1556',\n",
       " '20160121x1259',\n",
       " '20171213x1012',\n",
       " '20190710x1517',\n",
       " '20170920x1313',\n",
       " '20180110x1405',\n",
       " '20180605x1417',\n",
       " '20050209x1135']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which have duplicates?\n",
    "kept = delete_df[delete_df.status=='kept']\n",
    "kept = list(kept['session'])\n",
    "import collections\n",
    "dups = [item for item, count in collections.Counter(kept).items() if count > 1]\n",
    "dups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c2a9c-ca4a-40aa-972b-6b4555d14013",
   "metadata": {},
   "source": [
    "## Move ANTs analyses to new project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc1e7e-7abe-4649-99fb-4f6665c394db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 123894/123894x20190327x3T...\n",
      "Downloading from 100049/100049x20160621x3T...\n",
      "  [Errno 17] File exists: '/media/will/Samsung_T5/store/analyses/100049/100049x20160621x3T'\n",
      "Downloading from 100113/100113x20160620x3T...\n",
      "Downloading from 120937/120937x20160711x3T...\n",
      "Downloading from 119851/119851x20170606x3T...\n",
      "Downloading from 121105/121105x20170427x3T...\n",
      "Downloading from 122005/122005x20171213x3T...\n",
      "Downloading from 100957/100957_20110426...\n",
      "Downloading from 100551/20081008x1021...\n",
      "Downloading from 100978/20041103x1130...\n",
      "Downloading from 101841x02/20090106x1214...\n",
      "Downloading from 103610/20101220x1159...\n",
      "Downloading from 104190/20091109x1206...\n",
      "Downloading from 105223/20091103x1415...\n",
      "Downloading from 105644/20111103x1154...\n",
      "Downloading from 105769/20090316x1310...\n",
      "Downloading from 105785/20090512x1126...\n",
      "Downloading from 107457/20040928x1330...\n",
      "Downloading from 108790/20090218x1235...\n",
      "Downloading from 109198/20091201x1029...\n",
      "Downloading from 110445/20060726x1128...\n",
      "Downloading from 111006/20050705x1118...\n",
      "Downloading from 111642/20080108x1018...\n",
      "Downloading from 114753/20060607x1336...\n",
      "Downloading from 115001/20090331x0933...\n",
      "Downloading from 115264/20090304x1208...\n",
      "Downloading from 116504/20111220x1256...\n",
      "Downloading from 118011/20130612x1523...\n",
      "Downloading from 118410/20140226x1627...\n",
      "Downloading from 118460/20131108x1000...\n",
      "Downloading from 120430/20151007x1556...\n",
      "Downloading from 120691/20160121x1259...\n",
      "Downloading from 121277/20160907x1425...\n",
      "Downloading from 122242/20170810x1221...\n",
      "Downloading from 122417/20171213x1012...\n",
      "Downloading from 122601/20170920x1313...\n",
      "Downloading from 122821/20180110x1405...\n",
      "Downloading from 123352/20180605x1417...\n",
      "Downloading from 125098/20190710x1517...\n",
      "Downloading from 122216/122216x20200205x3T...\n",
      "Downloading from 105371/20071009x1020...\n",
      "Downloading from 105468/20060802x1113...\n"
     ]
    }
   ],
   "source": [
    "# download the analysis data from the collection\n",
    "ants_gear = 'antsct-aging-fw'\n",
    "for ses in collection_sessions:\n",
    "    adir = os.path.join('/media/will/Samsung_T5/store/analyses', ses.subject.label, ses.label)\n",
    "    print(f\"Downloading from {ses.subject.label}/{ses.label}...\")\n",
    "    try:\n",
    "        os.makedirs(adir)\n",
    "    except FileExistsError as e:\n",
    "        print('  ' + str(e))\n",
    "        continue\n",
    "    for asys in ses.analyses:\n",
    "        if (asys.gear_info.get('name') == ants_gear) and (asys.job['state']=='complete') and (asys.files):\n",
    "            # select file and download\n",
    "            antsct_output = [f for f in asys.files if f.name.endswith('zip')][0].name\n",
    "            asys.download_file(antsct_output, f'{adir}/{antsct_output}')\n",
    "            # record label in text output\n",
    "            with open(f'{adir}/label.txt', 'w') as f:\n",
    "                f.write(asys.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8d39a-acb2-4078-8088-3a74e14e8d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'acquisition', 'id': '619537efdd46e7e76fc3b392', 'name': '1.3.12.2.1107.5.2.43.66044.2019032710053479167074791.0.0.0.nii.gz'}\n",
      "/media/will/Samsung_T5/store/analyses/123894/123894x20190327x3T/antsct_sub-123894_ses-123894x20190327x3T.zip\n"
     ]
    }
   ],
   "source": [
    "# create analyses in Flywheel project\n",
    "for ses in project_sessions:\n",
    "    # get the input T1 file, which should already be in the session's acquisitions\n",
    "    acq = ses.acquisitions()[0] # should only be one after deletion\n",
    "    nii = [f for f in acq.files if f.name.endswith('.nii.gz')][0]\n",
    "    file_ref = acq.get_file(nii.name).ref()\n",
    "    # print(file_ref)    \n",
    "    \n",
    "    # create the analysis container \n",
    "    adir = os.path.join('/media/will/Samsung_T5/store/analyses', ses.subject.label, ses.label)\n",
    "    try:\n",
    "        with open (f\"{adir}/label.txt\", \"r\") as label: # read the label text\n",
    "            lab=label.readlines()\n",
    "        #os.remove(f\"{adir}/label.txt\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    if len(ses.analyses) < 1:\n",
    "        asys_label = lab[0]\n",
    "        asys = ses.add_analysis(label=asys_label,inputs=[file_ref])\n",
    "    \n",
    "        # upload the results zip file \n",
    "        zip_file = glob.glob(adir+\"/*.zip\")[0]\n",
    "        print(zip_file)\n",
    "        asys.upload_output(zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b049e8-e455-4041-8229-142547772dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
